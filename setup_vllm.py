#!/usr/bin/env python3
"""
Quick setup script for vLLM with DocuVerse.
This script helps you get started with open-source models quickly.
"""

import os
import sys
import subprocess
import json
from pathlib import Path

def check_requirements():
    """Check if basic requirements are met."""
    print("üîç Checking requirements...")
    
    requirements = {
        "python": (3, 8),
        "gpu": None,  # Will check nvidia-smi
        "disk_space": 10,  # GB minimum
    }
    
    # Check Python version
    python_version = sys.version_info
    if python_version < requirements["python"]:
        print(f"‚ùå Python {requirements['python'][0]}.{requirements['python'][1]}+ required")
        return False
    print(f"‚úÖ Python {python_version.major}.{python_version.minor}")
    
    # Check GPU (optional but recommended)
    try:
        result = subprocess.run(["nvidia-smi"], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ NVIDIA GPU detected")
            requirements["gpu"] = True
        else:
            print("‚ö†Ô∏è  No NVIDIA GPU detected (CPU inference will be slower)")
            requirements["gpu"] = False
    except FileNotFoundError:
        print("‚ö†Ô∏è  nvidia-smi not found (CPU inference will be slower)")
        requirements["gpu"] = False
    
    # Check disk space
    try:
        if os.name == 'nt':  # Windows
            import shutil
            free_gb = shutil.disk_usage('.').free / (1024**3)
        else:  # Unix-like systems
            statvfs = os.statvfs('.')
            free_gb = (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)
        
        if free_gb < requirements["disk_space"]:
            print(f"‚ö†Ô∏è  Low disk space: {free_gb:.1f}GB (need {requirements['disk_space']}GB+)")
        else:
            print(f"‚úÖ Disk space: {free_gb:.1f}GB available")
    except Exception as e:
        print(f"‚ö†Ô∏è  Could not check disk space: {e}")
    
    return True

def install_vllm():
    """Install vLLM and dependencies."""
    print("\nüì¶ Installing dependencies...")
    
    try:
        # Install basic dependencies first
        subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "requests", "torch", "transformers", "accelerate", "openai"
        ], check=True)
        print("‚úÖ Basic dependencies installed")
        
        # Check if we're on Windows
        if os.name == 'nt':
            print("\n‚ö†Ô∏è  Windows detected - vLLM installation can be complex")
            print("   vLLM has limited Windows support and requires specific build tools")
            
            choice = input("\nü§î Try installing vLLM anyway? (y/N): ").lower().strip()
            if choice not in ['y', 'yes']:
                print("\nüí° Alternative: Use Ollama or HuggingFace Transformers instead")
                print("   ‚Ä¢ Ollama: Download from https://ollama.com/")
                print("   ‚Ä¢ HuggingFace: Already installed with transformers")
                return "skip_vllm"
        
        # Try to install vLLM
        print("üì¶ Installing vLLM (this may take several minutes)...")
        try:
            subprocess.run([
                sys.executable, "-m", "pip", "install", "vllm"
            ], check=True, timeout=600)  # 10 minute timeout
            print("‚úÖ vLLM installed successfully")
            return True
        except subprocess.TimeoutExpired:
            print("‚è∞ vLLM installation timed out")
            return "timeout"
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install vLLM: {e}")
            
            if os.name == 'nt':
                print("\nüí° Windows vLLM Installation Alternatives:")
                print("   1. Use WSL2 (Windows Subsystem for Linux)")
                print("   2. Use Docker with vLLM image")
                print("   3. Use Ollama (easier on Windows)")
                print("   4. Use HuggingFace Transformers directly")
                return "windows_alternatives"
            else:
                print("üí° Try installing manually:")
                print("   pip install vllm")
                return False
                
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to install basic dependencies: {e}")
        return False

def download_model(model_name="microsoft/DialoGPT-large"):
    """Download a model using HuggingFace."""
    print(f"\n‚¨áÔ∏è  Downloading model: {model_name}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        print("üì• Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print("üì• Downloading model (this may take a while)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype="auto",
            device_map="auto"
        )
        
        print(f"‚úÖ Model {model_name} downloaded successfully")
        return True
        
    except Exception as e:
        print(f"‚ùå Failed to download model: {e}")
        print("üí° You may need to log in to HuggingFace:")
        print("   huggingface-cli login")
        return False

def create_vllm_config():
    """Create a vLLM configuration file."""
    print("\nüìù Creating vLLM configuration...")
    
    config = {
        "model": "microsoft/DialoGPT-large",
        "host": "0.0.0.0",
        "port": 8000,
        "gpu_memory_utilization": 0.7,
        "max_model_len": 2048,
        "tensor_parallel_size": 1,
        "disable_log_stats": False
    }
    
    config_path = Path("vllm_config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ Configuration saved to {config_path}")
    return config

def create_ollama_config():
    """Create an Ollama configuration as alternative to vLLM."""
    print("\nüìù Creating Ollama configuration (vLLM alternative)...")
    
    config = {
        "provider": "ollama",
        "model": "llama2",
        "base_url": "http://localhost:11434",
        "temperature": 0.1,
        "max_tokens": 2048
    }
    
    config_path = Path("ollama_config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ Ollama configuration saved to {config_path}")
    return config

def create_huggingface_config():
    """Create a HuggingFace configuration as alternative to vLLM."""
    print("\nüìù Creating HuggingFace configuration (vLLM alternative)...")
    
    config = {
        "provider": "huggingface",
        "model_id": "microsoft/DialoGPT-large",
        "device": "auto",
        "quantization": "4bit",
        "temperature": 0.1,
        "max_tokens": 512
    }
    
    config_path = Path("huggingface_config.json")
    with open(config_path, "w") as f:
        json.dump(config, f, indent=2)
    
    print(f"‚úÖ HuggingFace configuration saved to {config_path}")
    return config

def create_start_script(config):
    """Create a script to start vLLM server."""
    print("\nüìú Creating start script...")
    
    script_content = f"""#!/bin/bash
# Start vLLM server
echo "üöÄ Starting vLLM server..."

python -m vllm.entrypoints.openai.api_server \\
    --model {config['model']} \\
    --host {config['host']} \\
    --port {config['port']} \\
    --gpu-memory-utilization {config['gpu_memory_utilization']} \\
    --max-model-len {config['max_model_len']} \\
    --tensor-parallel-size {config['tensor_parallel_size']}

echo "Server stopped"
"""
    
    # Windows batch script
    bat_content = f"""@echo off
echo üöÄ Starting vLLM server...

python -m vllm.entrypoints.openai.api_server ^
    --model {config['model']} ^
    --host {config['host']} ^
    --port {config['port']} ^
    --gpu-memory-utilization {config['gpu_memory_utilization']} ^
    --max-model-len {config['max_model_len']} ^
    --tensor-parallel-size {config['tensor_parallel_size']}

echo Server stopped
pause
"""
    
    # Create both scripts
    with open("start_vllm.sh", "w") as f:
        f.write(script_content)
    
    with open("start_vllm.bat", "w") as f:
        f.write(bat_content)
    
    # Make shell script executable
    try:
        os.chmod("start_vllm.sh", 0o755)
    except:
        pass
    
    print("‚úÖ Start scripts created:")
    print("   ‚Ä¢ start_vllm.sh (Linux/Mac)")
    print("   ‚Ä¢ start_vllm.bat (Windows)")

def create_test_script():
    """Create a test script for the vLLM setup."""
    print("\nüß™ Creating test script...")
    
    test_content = '''#!/usr/bin/env python3
"""Test script for vLLM setup."""

import requests
import json

def test_vllm_server():
    """Test if vLLM server is running and responding."""
    print("üß™ Testing vLLM server...")
    
    url = "http://localhost:8000/v1/chat/completions"
    
    payload = {
        "model": "microsoft/DialoGPT-large",
        "messages": [
            {
                "role": "user",
                "content": "Extract the contract value from this text: The total contract value is $50,000 per year."
            }
        ],
        "max_tokens": 100,
        "temperature": 0.1
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        
        print("‚úÖ vLLM server is working!")
        print(f"Response: {result['choices'][0]['message']['content']}")
        return True
        
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to vLLM server")
        print("üí° Make sure the server is running: ./start_vllm.sh")
        return False
        
    except Exception as e:
        print(f"‚ùå Error testing server: {e}")
        return False

if __name__ == "__main__":
    test_vllm_server()
'''
    
    with open("test_vllm.py", "w") as f:
        f.write(test_content)
    
    print("‚úÖ Test script created: test_vllm.py")

def show_next_steps(config):
    """Show next steps to the user."""
    print("\nüéâ Setup Complete!")
    print("=" * 50)
    
    print("\nüìã Next Steps:")
    print("1. Start the vLLM server:")
    if os.name == 'nt':  # Windows
        print("   start_vllm.bat")
    else:  # Linux/Mac
        print("   ./start_vllm.sh")
    
    print("\n2. Wait for the server to load (may take a few minutes)")
    
    print("\n3. Test the connection:")
    print("   python test_vllm.py")
    
    print("\n4. Use with DocuVerse:")
    print("   python llm_setup_example.py")
    
    print(f"\nüåê Server Details:")
    print(f"   URL: http://localhost:{config['port']}")
    print(f"   Model: {config['model']}")
    print(f"   API Format: OpenAI Compatible")
    
    print("\nüí° Troubleshooting:")
    print("   ‚Ä¢ Check GPU memory: nvidia-smi")
    print("   ‚Ä¢ Monitor server logs for errors")
    print("   ‚Ä¢ Try smaller models if OOM errors occur")
    print("   ‚Ä¢ Use CPU fallback: --device cpu")
    
    print("\nüîß Alternative Models:")
    print("   ‚Ä¢ mistralai/Mistral-7B-Instruct-v0.2 (fast)")
    print("   ‚Ä¢ microsoft/DialoGPT-large (smaller)")
    print("   ‚Ä¢ codellama/CodeLlama-7b-Instruct-hf (code)")

def create_ollama_scripts():
    """Create scripts to download and run Ollama models."""
    print("\nüìú Creating Ollama scripts...")
    
    # Ollama setup script for Windows
    setup_content = '''@echo off
echo ü§ñ Ollama Setup for DocuVerse
echo ================================

echo üì• Downloading and installing Ollama models...

REM Pull the model (this will download ~4GB)
ollama pull llama2

echo ‚úÖ Ollama setup complete!
echo.
echo üöÄ Starting Ollama server...
ollama serve
'''
    
    # Ollama test script
    test_content = '''#!/usr/bin/env python3
"""Test script for Ollama setup."""

import requests
import json

def test_ollama_server():
    """Test if Ollama server is running and responding."""
    print("üß™ Testing Ollama server...")
    
    url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": "llama2",
        "prompt": "Extract the contract value from this text: The total contract value is $50,000 per year.",
        "stream": False
    }
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        
        result = response.json()
        
        print("‚úÖ Ollama server is working!")
        print(f"Response: {result.get('response', 'No response')}")
        return True
        
    except requests.exceptions.ConnectionError:
        print("‚ùå Cannot connect to Ollama server")
        print("üí° Make sure Ollama is running: ollama serve")
        return False
        
    except Exception as e:
        print(f"‚ùå Error testing server: {e}")
        return False

if __name__ == "__main__":
    test_ollama_server()
'''
    
    # Create setup script
    with open("setup_ollama.bat", "w", encoding='utf-8') as f:
        f.write(setup_content)
    
    # Create test script
    with open("test_ollama.py", "w", encoding='utf-8') as f:
        f.write(test_content)
    
    print("‚úÖ Ollama scripts created:")
    print("   ‚Ä¢ setup_ollama.bat (Windows setup)")
    print("   ‚Ä¢ test_ollama.py (Test connection)")

def create_huggingface_scripts():
    """Create scripts for HuggingFace setup."""
    print("\nüìú Creating HuggingFace scripts...")
    
    test_content = '''#!/usr/bin/env python3
"""Test script for HuggingFace setup."""

def test_huggingface():
    """Test HuggingFace model loading."""
    print("üß™ Testing HuggingFace model loading...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        model_name = "microsoft/DialoGPT-large"
        
        print(f"üì• Loading tokenizer for {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        print(f"üì• Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        
        # Test generation
        test_prompt = "Extract the contract value: The total contract value is $50,000 per year."
        inputs = tokenizer.encode(test_prompt, return_tensors="pt")
        
        print("üîÆ Generating response...")
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 50,
                num_return_sequences=1,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"‚úÖ HuggingFace model is working!")
        print(f"Response: {response}")
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing HuggingFace: {e}")
        return False

if __name__ == "__main__":
    test_huggingface()
'''
    
    with open("test_huggingface.py", "w", encoding='utf-8') as f:
        f.write(test_content)
    
    print("‚úÖ HuggingFace test script created: test_huggingface.py")

def show_ollama_steps():
    """Show Ollama-specific next steps."""
    print("\nüéâ Ollama Setup Complete!")
    print("=" * 50)
    
    print("\nüìã Next Steps:")
    print("1. Install Ollama:")
    print("   ‚Ä¢ Download from: https://ollama.com/")
    print("   ‚Ä¢ Or run: winget install Ollama.Ollama")
    
    print("\n2. Run the setup script:")
    print("   setup_ollama.bat")
    
    print("\n3. Test the connection:")
    print("   python test_ollama.py")
    
    print("\n4. Use with DocuVerse:")
    print("   python llm_setup_example.py")
    
    print(f"\nüåê Ollama Details:")
    print(f"   URL: http://localhost:11434")
    print(f"   Model: llama2")
    print(f"   API Format: Ollama REST API")
    
    print("\nüí° Ollama Commands:")
    print("   ‚Ä¢ List models: ollama list")
    print("   ‚Ä¢ Pull model: ollama pull llama2")
    print("   ‚Ä¢ Start server: ollama serve")
    print("   ‚Ä¢ Chat: ollama run llama2")

def show_huggingface_steps():
    """Show HuggingFace-specific next steps."""
    print("\nüéâ HuggingFace Setup Complete!")
    print("=" * 50)
    
    print("\nüìã Next Steps:")
    print("1. Test the model loading:")
    print("   python test_huggingface.py")
    
    print("\n2. Use with DocuVerse:")
    print("   python llm_setup_example.py")
    
    print(f"\nü§ó HuggingFace Details:")
    print(f"   Model: microsoft/DialoGPT-large")
    print(f"   Mode: Direct model loading")
    print(f"   Device: Auto (GPU if available)")

def show_multiple_provider_steps():
    """Show steps for multiple providers."""
    print("\nüéâ Multi-Provider Setup Complete!")
    print("=" * 50)
    
    print("\nüìã You now have multiple LLM options:")
    print("1. ü§ñ Ollama (for easy local inference)")
    print("   ‚Ä¢ Install: https://ollama.com/")
    print("   ‚Ä¢ Setup: setup_ollama.bat")
    print("   ‚Ä¢ Test: python test_ollama.py")
    
    print("\n2. ü§ó HuggingFace (for direct model access)")
    print("   ‚Ä¢ Test: python test_huggingface.py")
    
    print("\n3. üöÄ Use with DocuVerse:")
    print("   python llm_setup_example.py")
    
    print("\nüí° Choose the provider that works best for your setup!")

def main():
    """Main setup function."""
    print("üöÄ DocuVerse LLM Setup")
    print("=" * 30)
    
    if not check_requirements():
        print("\n‚ùå Requirements not met. Please fix issues and try again.")
        return
    
    print("\nüìå This script will set up local LLM providers:")
    print("   ‚Ä¢ Install basic dependencies (requests, transformers, etc.)")
    print("   ‚Ä¢ Configure Ollama for easy local inference")
    print("   ‚Ä¢ Set up HuggingFace as backup option")
    print("   ‚Ä¢ Create test scripts and configuration files")
    print("   ‚Ä¢ Skip vLLM (complex Windows installation)")
    
    choice = input("\nü§î Continue with Ollama + HuggingFace setup? (Y/n): ").lower().strip()
    if choice in ['n', 'no']:
        print("Setup cancelled.")
        return
    
    # Install basic dependencies only
    print("\nüì¶ Installing basic dependencies...")
    try:
        subprocess.run([
            sys.executable, "-m", "pip", "install", 
            "requests", "torch", "transformers", "accelerate", "openai"
        ], check=True)
        print("‚úÖ Basic dependencies installed")
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to install dependencies: {e}")
        return
    
    # Set up Ollama and HuggingFace configurations
    print("\nüîÑ Setting up local LLM providers...")
    
    print("\nü§ñ Creating Ollama configuration (Recommended)...")
    ollama_config = create_ollama_config()
    create_ollama_scripts()
    
    print("\nü§ó Creating HuggingFace configuration (Backup)...")
    hf_config = create_huggingface_config()
    create_huggingface_scripts()
    
    # Create a unified test script
    create_unified_test_script()
    
    show_multiple_provider_steps()

def create_unified_test_script():
    """Create a unified test script for all providers."""
    print("\nüß™ Creating unified test script...")
    
    test_content = '''#!/usr/bin/env python3
"""Unified test script for all LLM providers."""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_all_providers():
    """Test all available LLM providers."""
    print("üß™ Testing all LLM providers...")
    print("=" * 40)
    
    results = {}
    
    # Test Ollama
    print("\\nü§ñ Testing Ollama...")
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            print("‚úÖ Ollama server is running")
            results["ollama"] = True
        else:
            print("‚ö†Ô∏è  Ollama server not responding")
            results["ollama"] = False
    except Exception as e:
        print(f"‚ùå Ollama not available: {e}")
        results["ollama"] = False
    
    # Test HuggingFace
    print("\\nü§ó Testing HuggingFace...")
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-large")
        print("‚úÖ HuggingFace models accessible")
        results["huggingface"] = True
    except Exception as e:
        print(f"‚ùå HuggingFace not available: {e}")
        results["huggingface"] = False
    
    # Test DocuVerse integration
    print("\\nüìö Testing DocuVerse integration...")
    try:
        from docuverse.core.config import LLMConfig, LLMProvider
        from docuverse.extractors.few_shot import FewShotExtractor
        
        # Test with Ollama config
        if results["ollama"]:
            config = LLMConfig(
                provider=LLMProvider.OLLAMA,
                ollama_base_url="http://localhost:11434",
                ollama_model="llama2",
                temperature=0.1
            )
            
            extractor = FewShotExtractor(llm_config=config)
            print("‚úÖ DocuVerse + Ollama integration ready")
        
        elif results["huggingface"]:
            config = LLMConfig(
                provider=LLMProvider.HUGGINGFACE,
                hf_model_id="microsoft/DialoGPT-large",
                temperature=0.1
            )
            
            extractor = FewShotExtractor(llm_config=config)
            print("‚úÖ DocuVerse + HuggingFace integration ready")
        
        else:
            print("‚ö†Ô∏è  No local providers available")
            
    except Exception as e:
        print(f"‚ùå DocuVerse integration error: {e}")
    
    # Summary
    print("\\nÔøΩ Summary:")
    print("=" * 20)
    for provider, status in results.items():
        status_icon = "‚úÖ" if status else "‚ùå"
        print(f"{status_icon} {provider.title()}: {'Ready' if status else 'Not available'}")
    
    if any(results.values()):
        print("\\nüéâ At least one provider is working! You're ready to use DocuVerse.")
    else:
        print("\\n‚ö†Ô∏è  No providers available. Check installation steps.")

if __name__ == "__main__":
    test_all_providers()
'''
    
    with open("test_all_providers.py", "w", encoding='utf-8') as f:
        f.write(test_content)
    
    print("‚úÖ Unified test script created: test_all_providers.py")

if __name__ == "__main__":
    main()
